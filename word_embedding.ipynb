{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "Word embedding is feature learning techniques in NLP where words or phrases from the vocabulary are mapped to vectors of real numbers. It translates a space with many dimensions per word to a space with a much lower dimension that vector based.\n",
    "\n",
    "A embedding matrix of weights will be learned during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sns.set()\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_len = 50\n",
    "\n",
    "(train_sequences,train_labels),(test_sequences,test_labels) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=vocab_size)\n",
    "\n",
    "n_sample = len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "['<START>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', '<UNK>', '<UNK>', 'story', 'direction', '<UNK>', 'really', '<UNK>', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '<UNK>', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '<UNK>', 'father', 'came', 'from', 'the', 'same', '<UNK>', '<UNK>', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', '<UNK>', 'with', 'this', 'film', 'the', '<UNK>', '<UNK>', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', '<UNK>', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '<UNK>', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', '<UNK>', '<UNK>', 'was', 'amazing', 'really', '<UNK>', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', '<UNK>', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '<UNK>', 'to', 'the', 'two', 'little', '<UNK>', 'that', 'played', 'the', '<UNK>', 'of', '<UNK>', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '<UNK>', '<UNK>', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', '<UNK>', 'up', 'are', 'such', 'a', 'big', '<UNK>', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', '<UNK>', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', '<UNK>', 'because', 'it', 'was', 'true', 'and', 'was', '<UNK>', 'life', 'after', 'all', 'that', 'was', '<UNK>', 'with', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "train_ex_sequence = train_sequences[0]\n",
    "print(train_ex_sequence)\n",
    "train_ex_text = [reverse_word_index[index] for index in train_ex_sequence]\n",
    "print(train_ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = pad_sequences(train_sequences,maxlen=max_len, truncating='post')\n",
    "test_sequences  = pad_sequences(test_sequences,maxlen=max_len, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num = 49\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "batch_num = np.int32(n_sample/batch_size + 1)\n",
    "print('batch num = %d' % batch_num)\n",
    "\n",
    "batch_indices = []\n",
    "for i in range(batch_num-1):\n",
    "    batch_indices.append(np.arange(batch_size*i,batch_size*(i+1)))\n",
    "batch_indices.append(np.arange(batch_size*i,n_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_onehot = tf.Session().run(tf.one_hot(tf.constant(train_sequences),depth=vocab_size,axis=-1))\n",
    "test_sequences_onehot  = tf.Session().run(tf.one_hot(tf.constant(test_sequences) ,depth=vocab_size,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost: 0.6933\n",
      "cost after epoch 1: 0.5669, train_acc: 0.7131, test_acc: 0.6954\n",
      "cost after epoch 2: 0.4812, train_acc: 0.7661, test_acc: 0.7363\n",
      "cost after epoch 3: 0.4613, train_acc: 0.7769, test_acc: 0.7374\n",
      "cost after epoch 4: 0.4507, train_acc: 0.7835, test_acc: 0.7359\n",
      "cost after epoch 5: 0.4434, train_acc: 0.7894, test_acc: 0.7343\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,max_len,vocab_size))\n",
    "y = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "w1 = tf.get_variable('embedding_weights',shape=(vocab_size,embedding_dim),dtype=tf.float32,\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "w2 = tf.get_variable('w_linear',shape=(max_len*embedding_dim,1),dtype=tf.float32,\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('b_linear',shape=(1,1),dtype=tf.float32,initializer=tf.zeros_initializer())\n",
    "\n",
    "e = tf.matmul(x,w1)\n",
    "e_f = tf.contrib.layers.flatten(e)\n",
    "z = tf.reshape(tf.matmul(e_f,w2) + b2,[-1])\n",
    "\n",
    "a = tf.nn.sigmoid(z)\n",
    "y_p = tf.cast(a > 0.5,tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_p,tf.cast(y,tf.int32)),tf.float32))\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "n_epoch = 5\n",
    "print_cost = True\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_val = sess.run(cost,feed_dict={x:train_sequences_onehot,y:train_labels})\n",
    "    print (\"initial cost: %.4f\" % (cost_val))\n",
    "    for i in range(n_epoch):\n",
    "        for batch_index in batch_indices:\n",
    "            sess.run(optimizer,feed_dict={x:train_sequences_onehot[batch_index,:,:],y:train_labels[batch_index]})\n",
    "            \n",
    "        if print_cost:\n",
    "            train_cost = sess.run(cost,feed_dict={x:train_sequences_onehot,y:train_labels})\n",
    "            train_acc = sess.run(accuracy,feed_dict={x:train_sequences_onehot, y:train_labels})\n",
    "            test_acc = sess.run(accuracy,feed_dict={x:test_sequences_onehot, y:test_labels})\n",
    "            print (\"cost after epoch %d: %.4f, train_acc: %.4f, test_acc: %.4f\" % (i+1,train_cost,train_acc,test_acc))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 3)             3000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 3,151\n",
      "Trainable params: 3,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 5s 214us/sample - loss: 0.6816 - acc: 0.5670 - val_loss: 0.6412 - val_acc: 0.6613\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 4s 161us/sample - loss: 0.5743 - acc: 0.7174 - val_loss: 0.5382 - val_acc: 0.7276\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 4s 158us/sample - loss: 0.5092 - acc: 0.7509 - val_loss: 0.5141 - val_acc: 0.7384\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 4s 150us/sample - loss: 0.4871 - acc: 0.7653 - val_loss: 0.5068 - val_acc: 0.7424\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 4s 152us/sample - loss: 0.4750 - acc: 0.7696 - val_loss: 0.5066 - val_acc: 0.7427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a523d6b00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "num_epochs = 5\n",
    "model.fit(train_sequences, train_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67168754]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_sentence = 'this movie is excellent'\n",
    "ex_sequence = [word_index[word] for word in ex_sentence.split()]\n",
    "ex_sequence = [ex_sequence]\n",
    "ex_sequence = pad_sequences(ex_sequence,maxlen=max_len, truncating='post')\n",
    "model.predict(ex_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize \n",
    "Run the following code and go to https://projector.tensorflow.org/ and upload the vecs.tsv and meta.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for i in range(1, vocab_size):\n",
    "    word = reverse_word_index[i]\n",
    "    embeddings = weights[i]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why word embedding?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
